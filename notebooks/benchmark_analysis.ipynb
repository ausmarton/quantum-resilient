{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec18bb01",
   "metadata": {},
   "source": [
    "# Quantum-Resilient Cryptography Benchmark Analysis\n",
    "\n",
    "This notebook provides interactive analysis and visualization of benchmark results for PQC and classical algorithms.\n",
    "\n",
    "Run `python run_benchmarks.py` first to generate `results/benchmark_results.json`, then execute the cells below.\n",
    "\n",
    "## Algorithms Tested\n",
    "\n",
    "### Classical Algorithms\n",
    "- RSA-2048, RSA-4096 (asymmetric encryption)\n",
    "- ECDSA-P256, ECDSA-P384 (digital signatures)\n",
    "- ECDH-P256, ECDH-P384 (key exchange)\n",
    "- AES-256 (symmetric encryption)\n",
    "\n",
    "### PQC Algorithms (NIST Standardized)\n",
    "- ML-KEM-512, ML-KEM-768, ML-KEM-1024 (key encapsulation)\n",
    "- ML-DSA-44, ML-DSA-65, ML-DSA-87 (digital signatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4771bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Notebook initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addb4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark results\n",
    "results_path = Path('../results/benchmark_results.json')\n",
    "\n",
    "if results_path.exists():\n",
    "    with open(results_path, 'r') as f:\n",
    "        results_data = json.load(f)\n",
    "    \n",
    "    df = pd.DataFrame(results_data)\n",
    "    print(f\"Loaded {len(df)} benchmark results\")\n",
    "    print(f\"Algorithms: {sorted(df['algorithm_name'].unique())}\")\n",
    "    print(f\"Data sizes: {sorted(df['data_size'].unique())}\")\n",
    "else:\n",
    "    print(\"No benchmark results found. Run the benchmarking script first: python ../run_benchmarks.py\")\n",
    "    # Create sample data for demonstration\n",
    "    df = pd.DataFrame({\n",
    "        'algorithm_name': ['RSA-2048', 'ML-KEM-512', 'ECDSA-P256', 'ML-DSA-44'],\n",
    "        'data_size': [1024, 1024, 1024, 1024],\n",
    "        'mean_latency_ms': [5.2, 8.1, 3.8, 12.5],\n",
    "        'throughput_ops_per_sec': [192, 123, 263, 80]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive algorithm type\n",
    "\n",
    "def categorize_algorithm(name: str) -> str:\n",
    "    return 'PQC' if ('ML-KEM' in name or 'ML-DSA' in name) else 'Classical'\n",
    "\n",
    "if 'algorithm_type' not in df.columns:\n",
    "    df['algorithm_name'] = df['algorithm_name'].astype(str)\n",
    "    df['algorithm_type'] = df['algorithm_name'].apply(categorize_algorithm)\n",
    "\n",
    "# Summary statistics\n",
    "summary = df.groupby(['algorithm_type', 'algorithm_name']).agg({\n",
    "    'mean_latency_ms': ['mean', 'std', 'min', 'max'],\n",
    "    'throughput_ops_per_sec': ['mean', 'std', 'min', 'max']\n",
    "}).round(2)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298fc0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency comparison (interactive)\n",
    "fig = px.box(df, x='algorithm_name', y='mean_latency_ms', color='algorithm_type',\n",
    "             title='Latency Distribution by Algorithm')\n",
    "fig.update_layout(xaxis_title='Algorithm', yaxis_title='Mean Latency (ms)')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e7a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughput vs data size (interactive)\n",
    "if 'data_size' in df.columns and 'throughput_ops_per_sec' in df.columns:\n",
    "    fig = px.scatter(df, x='data_size', y='throughput_ops_per_sec', color='algorithm_name',\n",
    "                     size='mean_latency_ms', title='Throughput vs Data Size', log_x=True, log_y=True)\n",
    "    fig.update_layout(xaxis_title='Data Size (bytes)', yaxis_title='Throughput (ops/sec)')\n",
    "    fig.show()\n",
    "else:\n",
    "    print('Throughput columns not present in the dataset.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff54bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PQC vs Classical comparison\n",
    "comp = df.groupby('algorithm_type').agg({'mean_latency_ms': 'mean', 'throughput_ops_per_sec': 'mean'}).round(2)\n",
    "comp_display = comp.copy()\n",
    "\n",
    "if set(['PQC', 'Classical']).issubset(set(comp.index)):\n",
    "    pqc_latency = comp.loc['PQC', 'mean_latency_ms']\n",
    "    classical_latency = comp.loc['Classical', 'mean_latency_ms']\n",
    "    overhead = ((pqc_latency - classical_latency) / classical_latency) * 100\n",
    "    print(f\"PQC overhead: {overhead:.1f}%\")\n",
    "else:\n",
    "    print('Not enough data to compute PQC vs Classical overhead.')\n",
    "\n",
    "comp_display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca365d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security level vs performance trade-off\n",
    "security_levels = {\n",
    "    'RSA-2048': 112, 'RSA-4096': 128,\n",
    "    'ECDSA-P256': 128, 'ECDSA-P384': 192,\n",
    "    'ECDH-P256': 128, 'ECDH-P384': 192,\n",
    "    'AES-256': 256,\n",
    "    'ML-KEM-512': 128, 'ML-KEM-768': 192, 'ML-KEM-1024': 256,\n",
    "    'ML-DSA-44': 128, 'ML-DSA-65': 192, 'ML-DSA-87': 256\n",
    "}\n",
    "\n",
    "df['security_level'] = df['algorithm_name'].map(security_levels)\n",
    "fig = px.scatter(df, x='security_level', y='mean_latency_ms', color='algorithm_type', size='throughput_ops_per_sec',\n",
    "                 hover_data=['algorithm_name'], title='Security Level vs Performance Trade-off')\n",
    "fig.update_layout(xaxis_title='Security Level (bits)', yaxis_title='Mean Latency (ms)')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45474b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple recommendations based on current data\n",
    "recs = []\n",
    "fastest = df.loc[df['mean_latency_ms'].idxmin()]\n",
    "recs.append(f\"Fastest algorithm: {fastest['algorithm_name']} ({fastest['mean_latency_ms']:.2f} ms)\")\n",
    "\n",
    "pqc_df = df[df['algorithm_type'] == 'PQC']\n",
    "if not pqc_df.empty:\n",
    "    best_pqc = pqc_df.loc[pqc_df['mean_latency_ms'].idxmin()]\n",
    "    recs.append(f\"Best PQC algorithm observed: {best_pqc['algorithm_name']} ({best_pqc['mean_latency_ms']:.2f} ms)\")\n",
    "\n",
    "for i, r in enumerate(recs, 1):\n",
    "    print(f\"{i}. {r}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

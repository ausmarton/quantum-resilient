# Quantum-Resilient Cryptography Benchmarking Framework
# Makefile for easy execution of benchmarks and analysis

.PHONY: help install test benchmark analyze clean results

# Default target
help:
	@echo "Quantum-Resilient Cryptography Benchmarking Framework"
	@echo "=================================================="
	@echo ""
	@echo "Available targets:"
	@echo "  install     - Install Python dependencies"
	@echo "  test        - Run all tests"
	@echo "  benchmark   - Run comprehensive benchmarks"
	@echo "  analyze     - Run analysis and generate visualizations"
	@echo "  objective3  - Run Objective 3 (Comprehensive Benchmarking)"
	@echo "  objective4  - Run Objective 4 (Performance Comparison)"
	@echo "  notebook    - Start Jupyter notebook for interactive analysis"
	@echo "  clean       - Clean generated files"
	@echo "  results     - Show benchmark results summary"
	@echo "  help        - Show this help message"

# Install dependencies
install:
	@echo "Installing Python dependencies..."
	pip install -r requirements.txt
	@echo "âœ… Dependencies installed successfully"

# Run tests
test:
	@echo "Running tests..."
	pytest tests/ -v --cov=src --cov-report=html
	@echo "âœ… Tests completed"

# Run comprehensive benchmarks
benchmark:
	@echo "ğŸš€ Running comprehensive benchmarks..."
	python run_benchmarks.py
	@echo "âœ… Benchmarks completed"

# Run analysis and generate visualizations
analyze:
	@echo "ğŸ“Š Running analysis and generating visualizations..."
	python -c "
import asyncio
import sys
from pathlib import Path
sys.path.insert(0, str(Path('src')))
from python_orchestrator.benchmarking import ComprehensiveBenchmarker
import yaml

with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

benchmarker = ComprehensiveBenchmarker(config)
benchmarker.generate_visualizations()
print('âœ… Analysis completed')
"
	@echo "âœ… Analysis completed"

# Run Objective 3 (Comprehensive Benchmarking)
objective3:
	@echo "ğŸ¯ Running Objective 3: Comprehensive Benchmarking"
	python src/python_orchestrator/main.py --objective 3
	@echo "âœ… Objective 3 completed"

# Run Objective 4 (Performance Comparison)
objective4:
	@echo "ğŸ¯ Running Objective 4: Performance Comparison"
	python src/python_orchestrator/main.py --objective 4
	@echo "âœ… Objective 4 completed"

# Start Jupyter notebook
notebook:
	@echo "ğŸ““ Starting Jupyter notebook..."
	jupyter notebook notebooks/benchmark_analysis.ipynb

# Clean generated files
clean:
	@echo "ğŸ§¹ Cleaning generated files..."
	rm -rf results/
	rm -rf htmlcov/
	rm -rf .pytest_cache/
	rm -rf __pycache__/
	rm -rf src/__pycache__/
	rm -rf src/python_orchestrator/__pycache__/
	rm -rf src/real_world/__pycache__/
	rm -rf src/reporting/__pycache__/
	rm -rf tests/__pycache__/
	rm -rf tests/unit/__pycache__/
	rm -rf tests/benchmarks/__pycache__/
	rm -rf tests/integration/__pycache__/
	rm -f *.log
	@echo "âœ… Clean completed"

# Show benchmark results summary
results:
	@echo "ğŸ“Š Benchmark Results Summary"
	@echo "============================"
	@if [ -f "results/benchmark_results.json" ]; then \
		echo "âœ… Benchmark results found"; \
		echo "ğŸ“ Results directory: results/"; \
		echo "ğŸ“„ Files generated:"; \
		ls -la results/ 2>/dev/null || echo "   No results directory found"; \
	else \
		echo "âŒ No benchmark results found"; \
		echo "ğŸ’¡ Run 'make benchmark' to generate results"; \
	fi

# Run full pipeline (install, test, benchmark, analyze)
all: install test benchmark analyze results

# Quick benchmark (minimal iterations for testing)
quick-benchmark:
	@echo "âš¡ Running quick benchmark (reduced iterations)..."
	@cp config.yaml config.yaml.backup
	@sed -i 's/iterations: 1000/iterations: 10/g' config.yaml
	@sed -i 's/duration_seconds: 300/duration_seconds: 10/g' config.yaml
	python run_benchmarks.py
	@mv config.yaml.backup config.yaml
	@echo "âœ… Quick benchmark completed"

# Show system information
info:
	@echo "ğŸ” System Information"
	@echo "===================="
	@echo "Python version:"
	@python --version
	@echo ""
	@echo "Installed packages:"
	@pip list | grep -E "(pandas|numpy|matplotlib|plotly|scipy)"
	@echo ""
	@echo "Available algorithms:"
	@python -c "
import yaml
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)
classical = [algo['name'] for algo in config['algorithms']['classical']]
pqc = [algo['name'] for algo in config['algorithms']['pqc']]
print(f'Classical: {len(classical)} algorithms')
print(f'PQC: {len(pqc)} algorithms')
print(f'Total: {len(classical) + len(pqc)} algorithms')
"

# Docker targets
docker-build:
	@echo "ğŸ³ Building Docker image..."
	docker build -f docker/Dockerfile -t pqc-benchmark .

docker-run:
	@echo "ğŸ³ Running benchmarks in Docker..."
	docker run -v $(PWD)/results:/app/results pqc-benchmark

# Kubernetes targets
k8s-deploy:
	@echo "â˜¸ï¸  Deploying to Kubernetes..."
	kubectl apply -f k8s/

k8s-status:
	@echo "â˜¸ï¸  Kubernetes deployment status:"
	kubectl get pods -n quantumresilient

k8s-logs:
	@echo "â˜¸ï¸  Kubernetes logs:"
	kubectl logs -f deployment/quantumresilient -n quantumresilient

# Development targets
format:
	@echo "ğŸ¨ Formatting code..."
	black src/ tests/
	flake8 src/ tests/

lint:
	@echo "ğŸ” Running linter..."
	flake8 src/ tests/
	mypy src/

# Documentation
docs:
	@echo "ğŸ“š Building documentation..."
	cd docs && make html
	@echo "âœ… Documentation built in docs/_build/html/"

# Performance profiling
profile:
	@echo "ğŸ“ˆ Running performance profiling..."
	python -m cProfile -o profile.stats run_benchmarks.py
	@echo "âœ… Profiling completed. Use 'python -c \"import pstats; pstats.Stats('profile.stats').sort_stats('cumulative').print_stats(20)\"' to view results"

# Memory profiling
memory-profile:
	@echo "ğŸ§  Running memory profiling..."
	python -m memory_profiler run_benchmarks.py
	@echo "âœ… Memory profiling completed"
